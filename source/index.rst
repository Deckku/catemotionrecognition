catemotionrecognition documentation master file, created by
sphinx-quickstart on Thu Dec  5 23:02:11 2024.
You can adapt this file completely to your liking, but it should at least
contain the root `toctree` directive.

Welcome to cat emotion recognition Project
==========================================

Table des Mati√®res
==================
1. R√©sum√©
2. Introduction
3. Reconnaissance des √©motions du chat par la voix
4. Reconnaissance des √©motions du chat par l‚Äôimage
5. Reconnaissance de l‚Äô√©tat de sant√© du chat par l‚Äôimage
6. Combinaison des mod√®les
7. D√©ploiement
8. Conclusion

---

1. R√©sum√©
=========
Notre projet consiste √† √©laborer un mod√®le de machine learning capable de diagnostiquer l‚Äô√©tat et l‚Äôhumeur de l‚Äôanimal domestique, le chat, en temps r√©el. On a con√ßu 3 mod√®les. Le 1er permet de g√©n√©rer une pr√©diction sur l‚Äôhumeur √† partir des images captur√©es, le 2e consiste √† d√©tecter aussi l‚Äôhumeur, mais cette fois √† partir des audios comme input. Le 3e permet de faire une pr√©diction sur l‚Äô√©tat du chat √† partir des images, en identifiant si son √©tat est normal ou s‚Äôil est malade. Ensuite, on a combin√© ces mod√®les pour g√©n√©rer une pr√©diction sur des vid√©os r√©elles capt√©es du chat. Enfin, on a fait le d√©ploiement sur Streamlit et une application bureau.

---

2. Introduction
===============
Avec l‚Äôavanc√©e rapide des technologies en intelligence artificielle et en machine learning, de nouvelles opportunit√©s se pr√©sentent pour am√©liorer le bien-√™tre des animaux domestiques. Notre projet s‚Äôinscrit dans cette d√©marche en visant √† d√©velopper une solution innovante capable de diagnostiquer en temps r√©el l‚Äô√©tat de sant√© et l‚Äôhumeur d‚Äôun animal domestique, en particulier le chat. Ce travail repose sur l‚Äôanalyse de donn√©es multimodales telles que les images, les audios, et les vid√©os pour fournir des informations pr√©cises et utiles aux propri√©taires d‚Äôanimaux.
√Ä travers ce projet, nous avons con√ßu et combin√© plusieurs mod√®les de machine learning pour d√©tecter non seulement les √©motions du chat, mais aussi son √©tat de sant√©, et avons d√©ploy√© la solution sur des plateformes accessibles comme Streamlit et une application de bureau. L‚Äôobjectif final est d‚Äôoffrir un outil pratique et efficace permettant une interaction enrichie entre les propri√©taires et leurs animaux tout en veillant √† leur bien-√™tre.

---

3. Reconnaissance des √©motions du chat par la voix
==================================================

Guide de pr√©paration des donn√©es pour la reconnaissance d'√©motions par audio
----------------------------------------------------------------------------

Environnement : Google Colaboratory
Biblioth√®ques n√©cessaires : os, shutil, random

1. Collecte des donn√©es
-----------------------
La dataset utilis√©e comprend 5938 fichiers audio r√©partis en 10 classes :
['Angry', 'Defence', 'Fighting', 'Happy', 'HuntingMind', 'Mating', 'MotherCall', 'Paining', 'Resting', 'Warning'].
Cette dataset n'est pas directement disponible en ligne et a √©t√© obtenue via Monsieur Yagya Raj Pandeya.
On a trouv√© sur le plateforme kaggle une dataset de taille 100 audios de m√™me distribution de classe que l'on a laiss√© pour la phase de test.

2. Pr√©traitement des donn√©es
----------------------------
Le pr√©traitement des donn√©es inclut la division en ensembles d'entra√Ænement (80%) et de validation (20%).

.. code-block:: python

    import os
    import shutil
    import random
    import librosa
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import LabelEncoder

    # D√©finition des r√©pertoires
    source_dir = '/content/drive/MyDrive/NAYA_DATA_AUG1X'
    train_dir = '/content/drive/MyDrive/catemotionrecognitionbyaudio/datasets/train'
    val_dir = '/content/drive/MyDrive/catemotionrecognitionbyaudio/datasets/val'

    # Liste des classes
    def classes():
        return ['Angry', 'Defence', 'Fighting', 'Happy', 'HuntingMind',
                'Mating', 'MotherCall', 'Paining', 'Resting', 'Warning']

    # Fonction de r√©partition des donn√©es
    def split_data(source_dir, train_dir, val_dir, split_ratio=0.8):
        os.makedirs(train_dir, exist_ok=True)
        os.makedirs(val_dir, exist_ok=True)

        for class_name in classes():
            class_train_dir = os.path.join(train_dir, class_name)
            class_val_dir = os.path.join(val_dir, class_name)
            os.makedirs(class_train_dir, exist_ok=True)
            os.makedirs(class_val_dir, exist_ok=True)

            class_dir = os.path.join(source_dir, class_name)
            if not os.path.exists(class_dir):
                print(f"[AVERTISSEMENT] Le dossier {class_name} n'existe pas dans {source_dir}.")
                continue

            audio_files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]

            if not audio_files:
                print(f"[AVERTISSEMENT] Aucun fichier audio trouv√© dans {class_dir}.")
                continue

            random.shuffle(audio_files)
            train_size = int(len(audio_files) * split_ratio)
            train_files = audio_files[:train_size]
            val_files = audio_files[train_size:]

            for file in train_files:
                shutil.move(os.path.join(class_dir, file), os.path.join(class_train_dir, file))

            for file in val_files:
                shutil.move(os.path.join(class_dir, file), os.path.join(class_val_dir, file))

            print(f"[INFO] {len(train_files)} fichiers d√©plac√©s vers {class_train_dir}")
            print(f"[INFO] {len(val_files)} fichiers d√©plac√©s vers {class_val_dir}")

    # Appel de la fonction pour diviser les donn√©es
    split_data(source_dir, train_dir, val_dir)

3. Pr√©paration des donn√©es audio
---------------------------------
Nettoyage, transformation en spectrogrammes, normalisation et encodage des √©tiquettes.

.. code-block:: python

    def clean_audio(y, sr, low_freq=200, high_freq=8000):
        y_filtered = librosa.effects.preemphasis(y)
        stft = librosa.stft(y_filtered, n_fft=2048, hop_length=512)
        freqs = librosa.fft_frequencies(sr=sr, n_fft=2048)
        mask = (freqs >= low_freq) & (freqs <= high_freq)
        stft_filtered = stft[mask, :]
        return librosa.istft(stft_filtered, hop_length=512)

    def extract_spectrogram(y, sr, max_pad_len=128):
        try:
            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=sr / 2)
            log_S = librosa.power_to_db(S, ref=np.max)
            pad_width = max_pad_len - log_S.shape[1]
            return np.pad(log_S, ((0, 0), (0, pad_width)), mode='constant') if pad_width > 0 else log_S[:, :max_pad_len]
        except Exception as e:
            print(f"[ERREUR] Extraction du spectrogramme √©chou√©e : {e}")
            return None

    def load_audio_files(base_dir, max_pad_len=128):
        audio_data, labels = [], []
        for label in os.listdir(base_dir):
            folder_path = os.path.join(base_dir, label)
            if not os.path.isdir(folder_path):
                continue

            for file in os.listdir(folder_path):
                if file.endswith('.mp3'):
                    try:
                        y, sr = librosa.load(os.path.join(folder_path, file), sr=22050)
                        y_cleaned = clean_audio(y, sr)
                        spectrogram = extract_spectrogram(y_cleaned, sr, max_pad_len)
                        if spectrogram is not None:
                            audio_data.append(spectrogram)
                            labels.append(label)
                    except Exception as e:
                        print(f"[ERREUR] Erreur lors du traitement de {file}: {e}")

        return np.array(audio_data), np.array(labels)

Traitement des ensembles : entra√Ænement, validation et test
-----------------------------------------------------------

.. code-block:: python

    def process_data_for_all_sets(train_dir, val_dir, test_dir, max_pad_len=128):
        X_train, y_train = load_audio_files(train_dir, max_pad_len)
        X_val, y_val = load_audio_files(val_dir, max_pad_len)
        X_test, y_test = load_audio_files(test_dir, max_pad_len)

        mean, std = np.mean(X_train, axis=0), np.std(X_train, axis=0)
        X_train = (X_train - mean) / (std + 1e-8)
        X_val = (X_val - mean) / (std + 1e-8) if X_val.size > 0 else X_val
        X_test = (X_test - mean) / (std + 1e-8) if X_test.size > 0 else X_test

        label_encoder = LabelEncoder().fit(np.concatenate([y_train, y_val, y_test]))
        y_train = label_encoder.transform(y_train)
        y_val = label_encoder.transform(y_val) if y_val.size > 0 else []
        y_test = label_encoder.transform(y_test) if y_test.size > 0 else []

        return X_train, X_val, X_test, y_train, y_val, y_test, label_encoder

Visualisation de spectrogrammes al√©atoires
------------------------------------------

.. code-block:: python

    def display_random_spectrograms(data, labels, class_names, num_samples=5):
        indices = random.sample(range(len(data)), min(num_samples, len(data)))
        for idx in indices:
            plt.imshow(data[idx], aspect='auto', origin='lower', cmap='viridis')
            plt.title(f"Classe: {class_names[labels[idx]]}")
            plt.colorbar(format='%+2.0f dB')
            plt.show()

    # Exemple de traitement des donn√©es
    X_train, X_val, X_test, y_train, y_val, y_test, label_encoder = process_data_for_all_sets(
        train_dir, val_dir, '/content/drive/MyDrive/catemotionrecognitionbyaudio/datasets/test'
    )
    display_random_spectrograms(X_train, y_train, label_encoder.classes_)

Image des spectrogrammes combin√©s
---------------------------------

Voici une repr√©sentation visuelle des spectrogrammes combin√©s pour chaque classe¬†:

.. image:: 123.jpg
    :alt: Spectrogrammes combin√©s
    :width: 800px
    :align: center

### Construction des mod√®les
On a con√ßu un mod√®le CNN.

### √âvaluation des performances
Mesures et r√©sultats des tests effectu√©s sur le mod√®le.

---

4. Reconnaissance des √©motions du chat par l‚Äôimage
==================================================

1. Collecte des donn√©es
-----------------------
Les images n√©cessaires ont √©t√© collect√©es √† partir de diverses sources publiques et bases de donn√©es sp√©cialis√©es, garantissant une diversit√© de visages et d'expressions √©motionnelles. Des crit√®res d‚Äôinclusion sp√©cifiques, tels que la r√©solution et la qualit√© des images, ont √©t√© d√©finis pour assurer la pertinence des donn√©es. Des autorisations ont √©t√© respect√©es pour les sources publiques afin de garantir un usage √©thique.

2. Pr√©paration des donn√©es image
---------------------------------
Nettoyage, transformation en spectrogrammes, normalisation et encodage des √©tiquettes.

.. code-block:: python

    def load_and_preprocess_images(image_dir):
    images = []
    labels = []
    for class_idx, class_name in enumerate(CLASS_NAMES):
        class_dir = os.path.join(image_dir, class_name)
        for image_file in os.listdir(class_dir):
            image_path = os.path.join(class_dir, image_file)
            img = load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))
            img_array = img_to_array(img) / 255.0
            images.append(img_array)
            labels.append(class_idx)
    return np.array(images), to_categorical(np.array(labels), num_classes=NUM_CLASSES)



3. Construction du modele
---------------------------------
Une architecture CNN (Convolutional Neural Network) a √©t√© choisie pour ses performances √©prouv√©es dans le traitement d'images. Le mod√®le a √©t√© construit avec plusieurs couches convolutives suivies de couches de pooling et d‚Äôune couche dense finale. L‚Äôoptimisation a √©t√© r√©alis√©e √† l‚Äôaide de l‚Äôalgorithme Adam, et des fonctions d‚Äôactivation ReLU ont √©t√© utilis√©es.

.. code-block:: python

    def create_cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

4. Classes de Reconnaissance des √âmotions Bas√©e sur l'Image
--------------------------------------------------------

Dans le cadre de l'analyse des √©motions des chats √† partir d'images, nous avons utilis√© un mod√®le de r√©seau de neurones convolutifs (CNN) pour identifier les √©motions des chats √† partir de photos. Les classes d'√©motions que le mod√®le est capable de reconna√Ætre sont les suivantes :

1. **Angry (En col√®re)** : Cette √©motion indique que le chat se sent en col√®re, souvent en r√©ponse √† une menace ou une perturbation. Les signes de col√®re peuvent inclure des oreilles point√©es en arri√®re, des yeux dilat√©s et une posture tendue.

2. **Beg (Mendiant)** : Lorsque le chat cherche de l'attention ou de la nourriture, il peut adopter une posture de "mendicit√©". Cela inclut souvent une attitude de sollicitation, comme se frotter contre les jambes ou regarder fixement en direction de la nourriture.

3. **Annoyed (Agac√©)** : Un chat peut √™tre agac√© lorsqu'il est d√©rang√©, m√™me de mani√®re subtile. Les signes incluent souvent une posture tendue ou un regard distant.

4. **Frightened (Effray√©)** : Lorsque le chat ressent de la peur, il peut se figer, ses yeux deviennent plus larges et ses oreilles se replient. Il essaie souvent d'√©viter la situation qui provoque cette peur.

5. **Happy (Heureux)** : Un chat heureux est souvent d√©tendu, avec des yeux mi-clos et une posture douce. Le ronronnement est aussi un indicateur cl√© d'une √©motion positive chez les chats.

6. **Normal (Normal)** : Un chat dans un √©tat √©motionnel "normal" montre des signes de calme et de confort. Il peut √™tre dans une posture d√©tendue sans aucune indication de stress ou d'agression.

7. **Sad (Triste)** : Un chat triste peut montrer des signes de d√©pression ou de d√©sint√©r√™t, comme une posture affaiss√©e, des yeux mi-clos ou une perte d'app√©tit.

8. **Scared (Effray√©)** : Similaire √† "Frightened", mais souvent avec un sentiment plus intense. Un chat effray√© peut √™tre plus r√©actif et chercher √† fuir.

9. **Under the Weather (Pas bien)** : Un chat "pas bien" peut sembler l√©thargique, avoir une posture plus repli√©e, et montrer moins d'int√©r√™t pour son environnement, ce qui peut indiquer qu'il est malade ou fatigu√©.

10. **Curious (Curieux)** : Un chat curieux montre souvent des signes d'exploration, comme une attention accrue aux nouveaux objets ou environnements, avec des oreilles en avant et une posture droite.

11. **Playful (Joueur)** : Un chat joueur adopte une posture excit√©e, souvent avec les pattes avant √©tendues ou en train de sauter autour d'un objet, montrant une √©nergie ludique et curieuse.

Ces classes sont utilis√©es pour d√©terminer l'√©tat √©motionnel g√©n√©ral du chat √† partir de ses expressions faciales et de son comportement visible sur les images. L'objectif est de mieux comprendre le bien-√™tre des chats et d'offrir une m√©thode non intrusive pour observer leurs √©motions.


5. Reconnaissance de l‚Äô√©tat de sant√© du chat par l‚Äôimage
==========================================================

1. Collecte des donn√©es
-----------------------

Chaque image est lue, redimensionn√©e, normalis√©e et √©tiquet√©e avec une cat√©gorie sp√©cifique correspondant √† l'√©tat de sant√©.

2. Construction du modele
---------------------------------
Une architecture CNN (Convolutional Neural Network) a √©t√© choisie pour ses performances √©prouv√©es dans le traitement d'images. Le mod√®le a √©t√© construit avec plusieurs couches convolutives suivies de couches de pooling et d‚Äôune couche dense finale. L‚Äôoptimisation a √©t√© r√©alis√©e √† l‚Äôaide de l‚Äôalgorithme Adam, et des fonctions d‚Äôactivation ReLU ont √©t√© utilis√©es.

.. code-block:: python

    def create_cnn_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model



Dans ce cas, les images doivent √™tre √©tiquet√©es pour deux classes principales :

**Sick** : Images repr√©sentant un chat malade, avec des signes visibles de maladie.
**Healthy** : Images repr√©sentant un chat en bonne sant√©, sans signes de maladie.

Les images peuvent provenir de diverses sources, mais elles doivent √™tre √©tiquet√©es avec pr√©cision pour garantir la performance du mod√®le. Le pr√©traitement des donn√©es reste similaire √† ce qui a √©t√© d√©crit pr√©c√©demment, en redimensionnant et en normalisant les images.




---


6. Combinaison des mod√®les 
==========================

Le but de cette √©tape est de combiner les trois mod√®les de traitement d'image pour offrir une solution compl√®te de classification vid√©o. Ces mod√®les incluent :

**Mod√®le de reconnaissance des √©motions du chat** : Ce mod√®le identifie les √©motions du chat √† partir d'images fixes, telles que l'angoisse, la joie, la peur, etc.
**Mod√®le de reconnaissance de l'√©tat de sant√© du chat** : Ce mod√®le est charg√© de d√©terminer si le chat est malade ou en bonne sant√© en analysant des images.
**Mod√®le de traitement vid√©o** : Cette partie combine les pr√©dictions des deux premiers mod√®les sur chaque image d'une vid√©o pour fournir des r√©sultats dynamiques (sur les √©motions et l'√©tat de sant√©) tout au long de la s√©quence vid√©o.

Strat√©gies pour combiner les mod√®les:

Le d√©fi ici est d'int√©grer ces deux mod√®les (√©motions et sant√©) dans une cha√Æne de traitement vid√©o. Voici les principales √©tapes de la combinaison des mod√®les :

Extraction des images vid√©o : Pour analyser une vid√©o, il faut d'abord en extraire les images (frames). Ces images sont ensuite envoy√©es aux deux mod√®les pour obtenir des pr√©dictions individuelles.

Traitement par le mod√®le d'√©motions : Chaque image extraite de la vid√©o est envoy√©e au mod√®le de reconnaissance des √©motions. Le mod√®le g√©n√®re une pr√©diction sur l'√©motion du chat √† partir de l'image.

Traitement par le mod√®le de sant√© : La m√™me image est ensuite envoy√©e au mod√®le de reconnaissance de l'√©tat de sant√©, qui pr√©dit si le chat est malade ou non.

Fusion des r√©sultats : Les pr√©dictions des deux mod√®les peuvent √™tre combin√©es pour donner un aper√ßu global de l'√©tat de sant√© et des √©motions du chat pendant la vid√©o. Cela pourrait se faire par :

Moyenne ou pond√©ration des r√©sultats des deux mod√®les pour une prise de d√©cision finale par frame.

Affichage des r√©sultats combin√©s √† chaque frame sous forme d'annotations (par exemple, afficher √† la fois l'√©motion du chat et son √©tat de sant√©).

Suivi dynamique dans la vid√©o : En utilisant une fen√™tre temporelle (par exemple, sur plusieurs frames), vous pouvez suivre l'√©volution des √©motions et de l'√©tat de sant√© du chat au cours du temps. Cette approche peut √™tre utilis√©e pour d√©tecter des changements dans les √©motions ou l'√©tat de sant√© du chat dans la vid√©o.

.. code-block:: python

    def process_video(video_path):
        try:
            # Process video frame
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                messagebox.showerror("Error", "Could not open video file")
                return None, None, None, None

            ret, frame = cap.read()
            if not ret:
                messagebox.showerror("Error", "Could not read video frame")
                return None, None, None, None

            temp_frame = os.path.join(os.path.dirname(video_path), "temp_frame.jpg")
            cv2.imwrite(temp_frame, frame)
            
            # Process the frame for visual analysis
            emotion, sickness, confidence = process_image(temp_frame)
            if os.path.exists(temp_frame):
                os.remove(temp_frame)
            cap.release()

            # Extract and process audio
            audio_mood = None
            temp_audio = extract_audio(video_path)
            if temp_audio:
                audio_mood = process_audio(temp_audio)
                if os.path.exists(temp_audio):
                    os.remove(temp_audio)
            else:
                messagebox.showwarning("Warning", "Could not analyze audio. Make sure the video contains audio.")

            if emotion and sickness:
                return emotion, sickness, confidence, audio_mood
            return None, None, None, None

        except Exception as e:
            messagebox.showerror("Error", f"Failed to process video: {str(e)}")
            return None, None, None, None

7. D√©ploiement
==============
1. Vue d'ensemble
-----------------

L'application **Cat Mood & Health Analyzer** a √©t√© cr√©√©e en utilisant **Tkinter**, une biblioth√®que Python pour la cr√©ation d'interfaces graphiques. L'interface est con√ßue pour √™tre simple et intuitive, permettant √† l'utilisateur de t√©l√©charger des fichiers multim√©dia (images, vid√©os, ou audio) via un bouton de s√©lection. Lorsqu'un fichier est t√©l√©charg√©, l'application traite le contenu √† l'aide de mod√®les pr√©-entrain√©s pour analyser les √©motions, l'√©tat de sant√© et l'humeur du chat. Les r√©sultats sont affich√©s dans l'interface, accompagn√©s d'un message personnalis√©. L'application comprend √©galement un syst√®me de gestion des erreurs pour informer l'utilisateur de tout probl√®me de traitement. Enfin, l'application peut √™tre convertie en un ex√©cutable autonome √† l'aide de **PyInstaller** pour une utilisation sans installation pr√©alable de Python.

2. Aper√ßu
---------

Le Cat Mood & Health Analyzer est une application conviviale con√ßue pour analyser l'humeur et l'√©tat de sant√© des chats √† partir de divers fichiers multim√©dias, tels que des images, des vid√©os et des enregistrements audio. En utilisant des mod√®les d'apprentissage automatique avanc√©s, l'outil offre des informations sur les √©motions et le bien-√™tre de votre chat. L'application utilise trois mod√®les principaux :

1. **Mod√®le de reconnaissance des √©motions** : Ce mod√®le pr√©dit l'√©tat √©motionnel d'un chat √† partir des images, en le classant dans des cat√©gories telles que heureux, triste, en col√®re ou joueur.
2. **Mod√®le de d√©tection de maladie** : En utilisant des images, ce mod√®le √©value si le chat est en bonne sant√© ou potentiellement malade, en fournissant une classification "normal" ou "malade".
3. **Mod√®le d'analyse de l'humeur audio** : Ce mod√®le traite les fichiers audio, tels que les miaulements ou les ronronnements, pour d√©tecter diff√©rents √©tats ou humeurs, tels que la col√®re, la joie et la douleur.

En t√©l√©chargeant une photo, une vid√©o ou un enregistrement audio d'un chat, les utilisateurs peuvent recevoir une analyse d√©taill√©e comprenant l'√©tat √©motionnel du chat, son √©tat de sant√© et son humeur audio, aidant ainsi les propri√©taires √† mieux comprendre les besoins de leur chat. L'interface graphique intuitive facilite l'interaction, et elle fournit des r√©sultats en temps r√©el avec des retours d√©taill√©s bas√©s sur l'analyse.

3. Fonctionnalit√©s
------------------

- **Analyse d'image** : T√©l√©chargez une image nette de votre chat pour la pr√©diction de l'√©motion et de la sant√©.
- **Analyse vid√©o** : T√©l√©chargez des vid√©os, et l'application traitera √† la fois le contenu visuel et audio pour fournir une analyse plus compl√®te.
- **Analyse audio** : T√©l√©chargez des fichiers audio des sons de votre chat pour la d√©tection de l'humeur.
- **Interface conviviale** : Le design simple et moderne le rend accessible √† tous les amoureux des chats, des propri√©taires occasionnels aux professionnels.
- **Retour en temps r√©el** : Recevez imm√©diatement un retour sur l'√©tat √©motionnel et de sant√© de votre chat avec des messages faciles √† comprendre.

4. Commencer
------------

1. Lancez l'application.
2. T√©l√©chargez une image, une vid√©o ou un fichier audio de votre chat.
3. L'application analysera le fichier et fournira un rapport d√©taill√© sur l'humeur, l'√©tat de sant√© et les sons associ√©s de votre chat.
4. Si une vid√©o est t√©l√©charg√©e, les analyses visuelles et audio seront trait√©es.
5. Consultez les r√©sultats et utilisez les informations pour mieux prendre soin de votre chat.


.. code-block:: python

    import tkinter as tk
    from tkinter import filedialog, messagebox, ttk
    import numpy as np
    import cv2
    import librosa
    from tensorflow.keras.models import load_model
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from PIL import Image, ImageTk
    import os
    import subprocess

    # Constants
    IMG_HEIGHT, IMG_WIDTH = 128, 128
    IMAGE_CLASSES = ["angry", "beg", "annoyed", "frightened", "happy", "normal", "sad", "scared", "under the weather", "curious", "playful"]
    SICKNESS_CLASSES = ["normal", "sick"]
    AUDIO_CLASSES = ['Angry', 'Defence', 'Fighting', 'Happy', 'HuntingMind', 'Mating', 'MotherCall', 'Paining', 'Resting', 'Warning']

    # Load models
    try:
        image_model = load_model('catemotionrecognition\\progress\\maybe3.h5')
        sickness_model = load_model('catemotionrecognition\\progress\\sicknormalf.h5')
        audio_model = load_model('catemotionrecognition\\progress\\issa.h5')
    except:
        messagebox.showerror("Error", "Could not load models. Please check if model files exist.")
        exit()

    def get_friendly_message(emotion, sickness, confidence, mood=None):
        if confidence < 0.1:
            return "I'm not confident this is a cat image, or the image might be unclear. Please try uploading a clearer picture of a cat! üê±"
        
        messages = {
            "happy": "Your cat seems to be in a great mood! ", "sad": "Aww, your cat might need some extra love and attention right now üíï",
            "angry": "Looks like someone woke up on the wrong side of the bed! üòæ", "sick": "Your cat might not be feeling well. Consider a vet visit! üè•",
            "normal": "Your cat appears to be healthy! üåü", "beg": "Your cat is begging for something. Maybe it's time for a treat! üçñ",
            "annoyed": "Your cat seems annoyed. It might need some space. üòí", "frightened": "Your cat seems frightened. Try to comfort it. üò®",
            "scared": "Your cat is scared. It might need some reassurance. üò±", "under the weather": "Your cat seems under the weather. Keep an eye on it. üåßÔ∏è",
            "curious": "Your cat is curious about something. Let it explore! üïµÔ∏è", "playful": "Your cat is feeling playful. Time for some fun! üß∂"
        }
        
        audio_messages = {
            'Angry': "Your cat is expressing anger or frustration. They might need some space! üòæ", 'Defence': "Your cat is in a defensive mode - they might feel threatened üõ°Ô∏è",
            'Fighting': "Your cat is showing aggressive behavior - best to keep distance! ‚öîÔ∏è", 'Happy': "Your cat is expressing joy and contentment! üò∫",
            'HuntingMind': "Your cat is in hunting mode - they're feeling predatory! üêæ", 'Mating': "Your cat is making mating calls üíï",
            'MotherCall': "Your cat is making nurturing sounds, typical of mother cats! ü§±", 'Paining': "Your cat might be in pain or distress - consider a vet visit! üè•",
            'Resting': "Your cat is making peaceful, relaxed sounds üò¥", 'Warning': "Your cat is trying to warn about something - they might feel unsafe! ‚ö†Ô∏è"
        }
        
        base_message = f"I think your cat is feeling {emotion}. "
        health_message = "They appear to be healthy! üåü" if sickness == "normal" else "They might not be feeling well - consider a check-up! üè•"
        
        if mood:
            mood_message = f"\nBased on their vocalizations: {audio_messages.get(mood, f'They seem to be in a {mood.lower()} state.')}"

        return base_message + health_message

    def process_image(image_path):
        try:
            image = load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))
            img_array = img_to_array(image) / 255.0
            img_array = np.expand_dims(img_array, axis=0)

            emotion_predictions = image_model.predict(img_array, verbose=0)
            predicted_emotion = IMAGE_CLASSES[np.argmax(emotion_predictions)]
            confidence = np.max(emotion_predictions)

            sickness_predictions = sickness_model.predict(img_array, verbose=0)
            predicted_sickness = SICKNESS_CLASSES[np.argmax(sickness_predictions)]

            return predicted_emotion, predicted_sickness, confidence
        except Exception as e:
            messagebox.showerror("Error", f"Failed to process image: {str(e)}")
            return None, None, None

    def extract_audio(video_path):
        try:
            temp_audio = os.path.join(os.path.dirname(video_path), "temp_audio.wav")
            command = f"ffmpeg -i \"{video_path}\" -q:a 0 -map a \"{temp_audio}\" -y"
            subprocess.run(command, shell=True, check=True)
            return temp_audio
        except Exception as e:
            messagebox.showerror("Error", f"Audio extraction error: {str(e)}")
            return None

    def process_audio(audio_path):
        try:
            y, sr = librosa.load(audio_path, duration=3)
            mel_spect = librosa.feature.melspectrogram(y=y, sr=sr)
            mel_spect = librosa.power_to_db(mel_spect, ref=np.max)
            mel_spect = cv2.resize(mel_spect, (128, 128))
            mel_spect = np.expand_dims(mel_spect, axis=-1)
            mel_spect = np.expand_dims(mel_spect, axis=0)

            audio_predictions = audio_model.predict(mel_spect, verbose=0)
            predicted_mood = AUDIO_CLASSES[np.argmax(audio_predictions)]
            
            return predicted_mood
        except Exception as e:
            messagebox.showerror("Error", f"Failed to process audio: {str(e)}")
            return None

    def process_video(video_path):
        try:
            cap = cv2.VideoCapture(video_path)
            ret, frame = cap.read()
            if not ret:
                messagebox.showerror("Error", "Could not read video frame")
                return None, None, None, None

            temp_frame = os.path.join(os.path.dirname(video_path), "temp_frame.jpg")
            cv2.imwrite(temp_frame, frame)
            
            emotion, sickness, confidence = process_image(temp_frame)
            if os.path.exists(temp_frame):
                os.remove(temp_frame)
            cap.release()

            audio_mood = None
            temp_audio = extract_audio(video_path)
            if temp_audio:
                audio_mood = process_audio(temp_audio)
                if os.path.exists(temp_audio):
                    os.remove(temp_audio)

            if emotion and sickness:
                return emotion, sickness, confidence, audio_mood
            return None, None, None, None
        except Exception as e:
            messagebox.showerror("Error", f"Failed to process video: {str(e)}")
            return None, None, None, None

    def process_file():
        file_path = filedialog.askopenfilename(
            title="Select Your Cat Media",
            filetypes=[("Media Files", "*.png;*.jpg;*.jpeg;*.mp4;*.avi;*.mov;*.wav;*.mp3")]
        )
        
        if not file_path:
            return

        loading_label.config(text="Processing... Please wait! üê±")
        root.update()

        try:
            if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):
                emotion, sickness, confidence = process_image(file_path)
                if emotion and sickness:
                    friendly_message = get_friendly_message(emotion, sickness, confidence)
                    result_label.config(text=friendly_message, wraplength=500)
                    audio_result_label.config(text="No audio analysis available for images")
                    
                    image = Image.open(file_path)
                    image.thumbnail((300, 300))
                    photo = ImageTk.PhotoImage(image)
                    screenshot_label.config(image=photo)
                    screenshot_label.image = photo
                    
            elif file_path.lower().endswith(('.mp4', '.avi', '.mov')):
                emotion, sickness, confidence, audio_mood = process_video(file_path)
                if emotion and sickness:
                    friendly_message = get_friendly_message(emotion, sickness, confidence, audio_mood)
                    result_label.config(text=friendly_message, wraplength=500)

                    if audio_mood:
                        audio_result_label.config(text=f"Audio Analysis: {audio_messages.get(audio_mood)} üîä", fg='#333333')
                    else:
                        audio_result_label.config(text="Audio analysis unavailable. Please ensure your video has audio.", fg='#666666')

                    cap = cv2.VideoCapture(file_path)
                    ret, frame = cap.read()
                    if ret:
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        image = Image.fromarray(frame_rgb)
                        image.thumbnail((300, 300))
                        photo = ImageTk.PhotoImage(image)
                        screenshot_label.config(image=photo)
                        screenshot_label.image = photo
                    cap.release()

            elif file_path.lower().endswith(('.wav', '.mp3')):
                audio_mood = process_audio(file_path)
                if audio_mood:
                    result_label.config(text="Audio Analysis Only", wraplength=500)
                    audio_result_label.config(text=f"{audio_messages.get(audio_mood)} üîä", fg='#333333')
                    screenshot_label.config(image='')

            loading_label.config(text="")
            
        except Exception as e:
            messagebox.showerror("Error", f"An error occurred: {str(e)}")
            loading_label.config(text="")

    root = tk.Tk()
    root.title("üê± Cat Mood & Health Analyzer")
    root.geometry("800x800")
    root.configure(bg='#f0f0f0')

    style = ttk.Style()
    style.configure('Custom.TButton', font=('Arial', 12))

    header = tk.Label(root, text="Cat Mood & Health Analyzer", font=("Arial", 24, "bold"), bg='#f0f0f0', fg='#333333')
    header.pack(pady=30)

    description = tk.Label(root, text="Upload a photo, video, or audio file of your cat to analyze their mood and health status!", font=("Arial", 12), bg='#f0f0f0', fg='#666666', wraplength=600)
    description.pack(pady=10)

    process_button = ttk.Button(root, text="Upload Cat Media üìÅ", command=process_file, style='Custom.TButton')
    process_button.pack(pady=20)

    loading_label = tk.Label(root, text="", font=("Arial", 12), bg='#f0f0f0', fg='#666666')
    loading_label.pack(pady=10)

    result_frame = tk.Frame(root, bg='#f0f0f0')
    result_frame.pack(pady=20, padx=50, fill='both', expand=True)

    result_label = tk.Label(result_frame, text="Your cat's analysis will appear here! üò∫", font=("Arial", 14), bg='#ffffff', fg='#333333', wraplength=500, pady=20, padx=20, relief='ridge', borderwidth=1)
    result_label.pack(pady=20)

    audio_result_label = tk.Label(result_frame, text="Audio analysis will appear here for videos and audio files! üîä", font=("Arial", 14), bg='#ffffff', fg='#666666', wraplength=500, pady=20, padx=20, relief='ridge', borderwidth=1)
    audio_result_label.pack(pady=20)

    screenshot_label = tk.Label(root, bg='#f0f0f0')
    screenshot_label.pack(pady=20)

    footer = tk.Label(root, text="Made with ‚ù§Ô∏è for cats everywhere", font=("Arial", 10), bg='#f0f0f0', fg='#999999')
    footer.pack(pady=20)

    root.mainloop()


---

8. Conclusion
=============

Le projet **Cat Mood & Health Analyzer** repr√©sente une avanc√©e int√©ressante dans l'analyse du bien-√™tre des animaux de compagnie, en particulier les chats. Gr√¢ce √† l'intelligence artificielle et aux mod√®les d'apprentissage profond, il offre une m√©thode automatis√©e pour √©valuer l'√©tat √©motionnel et de sant√© des chats √† partir de simples fichiers multim√©dia. 

√Ä l'avenir, nous envisageons d'√©largir les capacit√©s de cette application en ajoutant de nouvelles fonctionnalit√©s, telles que la d√©tection d'autres maladies ou comportements sp√©cifiques, et en affinant les mod√®les pour augmenter leur pr√©cision. Nous souhaitons √©galement int√©grer des fonctionnalit√©s permettant aux v√©t√©rinaires et propri√©taires d'animaux de suivre l'√©volution de la sant√© des chats au fil du temps.

L'application pourrait trouver des applications dans diverses industries, telles que la sant√© animale, les refuges pour animaux, et m√™me dans les maisons pour surveiller le bien-√™tre des animaux domestiques. En simplifiant l'analyse du comportement des chats, ce projet pourrait offrir un outil pr√©cieux pour am√©liorer la qualit√© de vie de nos compagnons f√©lins.


---

.. toctree::
   :maxdepth: 2
   :caption: Contents:


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`

.. toctree::
   :maxdepth: 2
   :caption: Table des Mati√®res
   r√©sum√©
   introduction
   reconnaissance_voix
   reconnaissance_image
   reconnaissance_sant√©
   combinaison
   d√©ploiement
   conclusion

